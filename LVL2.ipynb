{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LVL2 Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import external libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "#pd.options.mode.chained_assignment = None  # default='warn'; chained index warning\n",
    "import numpy as np\n",
    "import imp\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "#plotting\n",
    "%matplotlib notebook\n",
    "\n",
    "#import self-written libraries\n",
    "import LVL1\n",
    "import LVL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data Import Tasks\n",
    "#path to level 1 data\n",
    "Glacier=\"Wolverine\" #or \"Gulkana\"\n",
    "Station=\"990\" #or other elevations; this depends on the naming convention of input data\n",
    "timezone='America/Anchorage' #choose from pytz.all_timezones\n",
    "pth=r\"Q:/Project Data/GlacierData/Benchmark_Program/Data/\" +Glacier+ r\"/AllYears/Wx/LVL1/emily/\" + Glacier.lower()+\"_990_2017_15minLVL1.csv\"\n",
    "\n",
    "#time format and column\n",
    "Local_time_column_name='Local_time'\n",
    "date_format='%Y/%m/%d %H:%M'\n",
    "\n",
    "#directory to save output data\n",
    "save_dir=r\"Q:/Project Data/GlacierData/Benchmark_Program/Data/\" +Glacier+ r\"/AllYears/Wx/LVL2/emily/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in level1 CSV\n",
    "dat=pd.read_csv(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat['local_time']=pd.to_datetime(dat[Local_time_column_name], format= date_format)\n",
    "dat=dat.set_index('local_time') #set this local time as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Give names of columns containing temperature, and precipitation data, data for general averaging, and wind direction\n",
    "temp_columns=['Tpassive1', 'Tpassive2', 'TAspirated1', 'TAspirated2']\n",
    "primary_temp_column='TAspirated1'\n",
    "#unaspirated_temp_columns=['Tpassive1', 'Tpassive2']\n",
    "\n",
    "precip_columns=['StageCumulative', 'TPGCumulative']\n",
    "general_data_columns=['RelHum', 'WindSpeed', 'WindGustSpeed']\n",
    "wind_dir_column='WindDir'\n",
    "wind_speed_column='WindSpeed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incremental_precip_columns=[] #store names of incremental precip columns\n",
    "for precip_col in precip_columns:\n",
    "    col_name=precip_col.split(\"Cumulative\")[0]+\"Incremental\"\n",
    "    incremental_precip_columns.append(col_name)\n",
    "    dat[col_name]=dat[precip_col]-dat[precip_col].shift(1)\n",
    "    if not np.isnan(dat.StageCumulative[0]):\n",
    "        dat.ix[0, col_name]=0 #set first value to 0, not NAN IF the initial cumulative series was also not null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Gaps\n",
    " #### * 15 min data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Temperature - for primary sensor, fill gaps. \n",
    "#  for <3 len gap, fill with linear interpolation\n",
    "dat.loc[:,primary_temp_column]=dat[primary_temp_column].interpolate(method='linear', limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  for gaps >3, fill with average of passive temperature sensors, as long as the passive sensors agree\n",
    "\n",
    "#Store locations of long NAN gaps to be filled\n",
    "primary_temp_null_indx=dat[primary_temp_column].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#List alternate temp columns\n",
    "secondary_temp_columns=list(set(temp_columns)-set([primary_temp_column]))\n",
    "passive_temp_columns=[s for s in temp_columns if \"passive\" in s]\n",
    "\n",
    "#Subset data to alternate temperature columns\n",
    "secondary_temp_dat=dat[secondary_temp_columns].copy()\n",
    "#Calculate the mean of the secondary temp values\n",
    "secondary_temp_dat['temp_mean']=secondary_temp_dat[secondary_temp_columns].mean(axis=1)\n",
    "#Calculate standard dev. of secondary temp values\n",
    "secondary_temp_dat['temp_sd']=secondary_temp_dat[secondary_temp_columns].std(axis=1)\n",
    "#Calculate temperature difference between avg. of other temperatures\n",
    "secondary_temp_dat['temp_diff']=abs(dat[primary_temp_column]-secondary_temp_dat.mean(axis=1))\n",
    "#Calculate the median of secondary temp values\n",
    "secondary_temp_dat['temp_median']=secondary_temp_dat[secondary_temp_columns].median(axis=1)\n",
    "#Calculate mean of passive temperature sensors (if a sensor is missing, mean will be NAN)\n",
    "secondary_temp_dat['passive_average']=dat[passive_temp_columns].mean(axis=1, skipna=False)\n",
    "#Calculate difference of 2 passive sensors from one another\n",
    "secondary_temp_dat['passive_difference_between_sensors']=abs(dat[passive_temp_columns[0]]-dat[passive_temp_columns[1]])\n",
    "\n",
    "#Fill remaining gaps (>3 length) in primary timeseries with average of all other sensors\n",
    "dat.loc[dat[primary_temp_column].isnull(), primary_temp_column]=secondary_temp_dat.temp_mean[dat[primary_temp_column].isnull()]\n",
    "\n",
    "#Second round of interpolating small gaps\n",
    "dat[primary_temp_column]=dat[primary_temp_column].interpolate(method='linear', limit=3) #interpolate small gaps again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find places where passive differs from aspirated\n",
    "asp_more_than_2deg_diff_from_passivemean_idx=(abs(dat[primary_temp_column]- secondary_temp_dat.passive_average))>2 #difference between asp and passive mean > 2 deg\n",
    "\n",
    "#In places where the passive AGREE with eachother, but DISAGREE with the aspirated mean, set main aspirated T to mean of passive.\n",
    "passive_sensors_agree_with_eachother_2deg_idx=secondary_temp_dat.passive_difference_between_sensors<2 #passive sensors agree with eachother (<2 deg diff)\n",
    "dat.loc[asp_more_than_2deg_diff_from_passivemean_idx & passive_sensors_agree_with_eachother_2deg_idx, primary_temp_column]=secondary_temp_dat.loc[asp_more_than_2deg_diff_from_passivemean_idx & passive_sensors_agree_with_eachother_2deg_idx,'passive_average'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Final outlier strip\n",
    "dat[primary_temp_column]=LVL1.hampel(dat[primary_temp_column], k=7) #this may not be neccessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final 15 minute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_columns=[primary_temp_column] +incremental_precip_columns+general_data_columns #columns to include in output\n",
    "save_name=Glacier.lower()+ Station + \"_15min_\"+\"LVL2.csv\" #filename\n",
    "save_pth=os.path.join(save_dir, save_name)\n",
    "\n",
    "dat[out_columns].to_csv(save_pth) #save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A=secondary_temp_dat.temp_diff\n",
    "# A=A.rename(\"new_interp\")\n",
    "# B=dat.TAspirated1\n",
    "# newww=pd.concat([A, B], axis=1)\n",
    "# newww[newww.new_interp!= newww.TAspirated1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create Hourly Data -\n",
    "#Temperature\n",
    "hourly_dat=pd.DataFrame()\n",
    "for temp_col in temp_columns:\n",
    "    #Create WMO Mean Temp\n",
    "    hourly_dat[temp_col+\"_min\"]=dat[temp_col].resample('H').min()\n",
    "    hourly_dat[temp_col+ \"_max\"]=dat[temp_col].resample('H').max()\n",
    "    hourly_dat[temp_col+\"_WMO\"]=hourly_dat[[temp_col+\"_min\", temp_col+'_max']].mean(axis=1)\n",
    "    #Create USGS Mean Temp\n",
    "    hourly_dat[temp_col+\"_USGS\"]=dat[temp_col].resample('H').mean()\n",
    "    \n",
    "#Precipitation\n",
    "for incremental_precip_col in incremental_precip_columns:\n",
    "    hourly_dat[incremental_precip_col]=dat[incremental_precip_col].resample('H').sum() #all precip recieved during his hour\n",
    "    \n",
    "#Other Data Types (that can be aggregated with a simple mean)\n",
    "for general_data_col in general_data_columns:\n",
    "    hourly_dat[general_data_col]=dat[general_data_col].resample('H').mean()\n",
    "\n",
    "#Wind Direction - this process is for data that is logged as vector-averaged \n",
    "  # for data that is not, use LVL2.vector_average_wind_direction\n",
    "#Convert to raidans\n",
    "dat['wind_dir_cos']=np.cos(dat[wind_dir_column]*(np.pi/180))\n",
    "dat['wind_dir_sin']=np.sin(dat[wind_dir_column]*(np.pi/180))\n",
    "\n",
    "#Calculate mean of x and y directions in radian space\n",
    "hourly_dat['wind_dir_cos']=dat.wind_dir_cos.resample('H').mean()\n",
    "hourly_dat['wind_dir_sin']=dat.wind_dir_sin.resample('H').mean()\n",
    "\n",
    "#Convert back to 0-360 coordinates\n",
    "hourly_dat[wind_dir_column]=(np.arctan2(hourly_dat.wind_dir_sin, hourly_dat.wind_dir_cos) * 180/np.pi) +180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_temp_columns = [s for s in hourly_dat.columns if primary_temp_column in s]\n",
    "\n",
    "out_columns=out_temp_columns+incremental_precip_columns+general_data_columns+ [wind_dir_column] #columns to include in output\n",
    "save_name=Glacier.lower()+ Station + \"_hourly_\"+\"LVL2.csv\" #filename\n",
    "save_pth=os.path.join(save_dir, save_name)\n",
    "\n",
    "hourly_dat[out_columns][:-1].to_csv(save_pth, float_format='%g') #write selected columns; omit last row (unlikely to be complete, with download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily_dat=pd.DataFrame() #create empty dataframe\n",
    "\n",
    "#Temperature\n",
    "for temp_col in temp_columns:\n",
    "    #Create WMO Mean Temp\n",
    "    daily_dat[temp_col+\"_min\"]=dat[temp_col].resample('D').min()\n",
    "    daily_dat[temp_col+ \"_max\"]=dat[temp_col].resample('D').max()\n",
    "    daily_dat[temp_col+\"_WMO\"]=daily_dat[[temp_col+\"_min\", temp_col+'_max']].mean(axis=1)\n",
    "    #Create USGS Mean Temp\n",
    "    daily_dat[temp_col+\"_USGS\"]=dat[temp_col].resample('D').mean()    \n",
    "    \n",
    "#Precipitation\n",
    "for incremental_precip_col in incremental_precip_columns:\n",
    "    daily_dat[incremental_precip_col]=dat[incremental_precip_col].resample('D').sum() #all precip recieved during his hour\n",
    "    \n",
    "#Other Data Types (that can be aggregated with a simple mean)\n",
    "for general_data_col in general_data_columns:\n",
    "    daily_dat[general_data_col]=dat[general_data_col].resample('D').mean()\n",
    "\n",
    "#Wind Direction - this process is for data that is logged as vector-averaged \n",
    "  # for data that is not, use LVL2.vector_average_wind_direction\n",
    "#Convert to raidans\n",
    "dat['wind_dir_cos']=np.cos(dat[wind_dir_column]*(np.pi/180))\n",
    "dat['wind_dir_sin']=np.sin(dat[wind_dir_column]*(np.pi/180))\n",
    "\n",
    "#Calculate mean of x and y directions in radian space\n",
    "daily_dat['wind_dir_cos']=dat.wind_dir_cos.resample('D').mean()\n",
    "daily_dat['wind_dir_sin']=dat.wind_dir_sin.resample('D').mean()\n",
    "\n",
    "#Convert back to 0-360 coordinates\n",
    "daily_dat[wind_dir_column]=(np.arctan2(daily_dat.wind_dir_sin, daily_dat.wind_dir_cos) * 180/np.pi) +180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_temp_columns = [s for s in daily_dat.columns if primary_temp_column in s]\n",
    "\n",
    "out_columns=out_temp_columns+incremental_precip_columns+general_data_columns+ [wind_dir_column] #columns to include in output\n",
    "save_name=Glacier.lower()+ Station + \"_daily_\"+\"LVL2.csv\" #filename\n",
    "save_pth=os.path.join(save_dir, save_name)\n",
    "\n",
    "daily_dat[out_columns][:-1].to_csv(save_pth, float_format='%g') #write selected columns; omit last row (unlikely to be complete, with download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wolverine990_daily_LVL2.csv'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q:/Project Data/GlacierData/Benchmark_Program/Data/Wolverine/AllYears/Wx/LVL2/emily/'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
